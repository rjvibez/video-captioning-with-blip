
# ğŸ¥ Video Captioning with BLIP ğŸ§ 

This project uses a Vision-Language Model (VLM) to automatically generate natural language descriptions from videos. It extracts key frames from `.mp4` videos and uses Salesforce's BLIP model to generate accurate and concise captions.

ğŸ”— **GitHub Repository**: [video-captioning-with-blip](https://github.com/rjvibez/video-captioning-with-blip)

---

## ğŸ§  Model Used

- **Model**: [`Salesforce/blip-image-captioning-base`](https://huggingface.co/Salesforce/blip-image-captioning-base)
- **Framework**: Hugging Face Transformers


---

## ğŸ“‚ Project Structure

```
video-captioning-with-blip/
â”œâ”€â”€ blip_video_captioning.py     # Main captioning script
â”œâ”€â”€ requirements.txt             # Dependency list
â”œâ”€â”€ *.mp4                        # Your input videos
â”œâ”€â”€ *_frame*.jpg                 # Extracted video frames
â””â”€â”€ README.md                    # Project documentation
```

---

## ğŸš€ How It Works

1. Loads BLIP image captioning model from Hugging Face.
2. For each video in the list:
   - Captures 3 frames from the video (at 10%, 50%, 90% of duration).
   - Saves and loads those frames using OpenCV + PIL.
   - Feeds the frames to the BLIP model for captioning.
   - Combines all 3 frame captions into a final description.

---

## ğŸ§ª Example Output

**Video**: `cutting_onion.mp4`  
```
ğŸ–¼ï¸ Frame 1 Caption: a person chopping vegetables on a cutting board  
ğŸ–¼ï¸ Frame 2 Caption: a close up of onions being chopped  
ğŸ–¼ï¸ Frame 3 Caption: a wooden cutting board with onions  
âœ… Final Caption: a person chopping vegetables on a cutting board | a close up of onions being chopped | a wooden cutting board with onions
```

**Video**: `eye_makeup.mp4`  
```
ğŸ–¼ï¸ Frame 1 Caption: a woman applying her makeup with a brush  
ğŸ–¼ï¸ Frame 2 Caption: a person looking in the mirror  
ğŸ–¼ï¸ Frame 3 Caption: a closeup of the eye  
âœ… Final Caption: a woman applying her makeup with a brush | a person looking in the mirror | a closeup of the eye
```

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/rjvibez/video-captioning-with-blip.git
cd video-captioning-with-blip
```

### 2. (Optional) Create Virtual Environment
```bash
python -m venv caption_env
caption_env\Scripts\activate   # On Windows
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Run the Script
Ensure your `.mp4` files are in the root folder and then run:

```bash
python blip_video_captioning.py
```

---

## ğŸ“¦ Dependencies

Install all required packages using:

```bash
pip install -r requirements.txt
```

If `requirements.txt` is missing, here are the key packages:

```txt
transformers
torch
opencv-python
Pillow
```

---

## ğŸ¤– Approach Overview

The script processes each video by:
- Extracting three key frames (start, middle, end)
- Captioning each frame using BLIP
- Combining captions for a more complete summary

This improves accuracy and relevance by leveraging multiple temporal perspectives.

---

## ğŸ§  Reflection

In some cases (e.g., `cutting_onion.mp4`), the model generated highly relevant captions. However, in others (like `eye_makeup.mp4`), the captions were vague. Manual labeling often outperformed the model in creativity and specificity, especially when the video context required deeper reasoning.

---

## ğŸ“œ License

This project is open-source and for educational purposes.

---

## ğŸ‘¤ Author

Developed by [@rjvibez](https://github.com/rjvibez)
